---
description: Patterns for managing inter-code dependencies in GitHub Actions workflows during single production workflow generation.
alwaysApply: false
---

## Single Dependency Pattern

**When Code Type A depends on Code Type B** (e.g., Terraform depends on Python Lambda):

### Upstream Job (Code Type B - Python Build)

- Build and package artifacts (e.g., Lambda zip file)
- Upload artifacts using `actions/upload-artifact@v4`:
  ```yaml
  - name: Upload Lambda package
    uses: actions/upload-artifact@v4
    with:
      name: lambda-package
      path: lambda-package.zip
      retention-days: 1
  ```

### Downstream Job (Code Type A - Terraform Deploy)

- **CRITICAL**: Configure job dependency using `needs:` to wait for upstream deploy job:
  ```yaml
    terraform-deploy:
    needs:
      [terraform-lint, terraform-security, terraform-validate, python-deploy]
    runs-on: ubuntu-latest
    environment: production
    steps:
      - uses: actions/checkout@v4
  ```
- **CRITICAL**: Download artifacts from upstream build job BEFORE any Terraform operations (with error handling):

  ```yaml
  - name: Download Lambda package from Python build job
    uses: actions/download-artifact@v4
    continue-on-error: true
    id: download-artifact
    with:
      name: lambda-package
      path: ./lambda-package

  - name: Verify artifact downloaded successfully
    if: steps.download-artifact.outcome != 'success'
    run: |
      echo "Error: Failed to download artifact 'lambda-package' from Python build job"
      exit 1
  ```

- **Place artifact in correct location** where Terraform expects it:
  ```yaml
  - name: Move Lambda package to Terraform directory
    run: |
      # CRITICAL: Check the actual path referenced in Terraform code
      # If Terraform uses: filename = "lambda_function.zip" in the same directory
      # Place it where Terraform expects it (e.g., iac/terraform/lambda_function.zip)
      mkdir -p ./iac/terraform
      cp ./lambda-package/lambda-package.zip ./iac/terraform/lambda_function.zip
      # Adjust path based on actual Terraform code location and filename
      echo "TF_VAR_lambda_package_path=$(pwd)/iac/terraform/lambda_function.zip" >> $GITHUB_ENV
  ```
- **Verify artifact exists** before Terraform operations:
  ```yaml
  - name: Verify Lambda package exists
    run: |
      # Verify the exact path that Terraform code references
      TERRAFORM_LAMBDA_PATH="./iac/terraform/lambda_function.zip"
      if [ ! -f "$TERRAFORM_LAMBDA_PATH" ]; then
        echo "Error: Lambda package not found at: $TERRAFORM_LAMBDA_PATH"
        echo "Downloaded artifacts location:"
        ls -la ./lambda-package/ || echo "lambda-package directory not found"
        exit 1
      fi
      echo "✓ Lambda package verified at: $TERRAFORM_LAMBDA_PATH"
  ```
- **Order of operations**: Checkout → Download artifacts → Place artifacts → Verify artifacts → Configure credentials → Terraform init/plan/apply

---

## Multiple Dependencies Pattern

If a code type depends on multiple others, configure job dependencies using `needs:` to wait for all upstream deploy jobs:

```yaml
  kubernetes-deploy:
  needs:
    [
      kubernetes-lint,
      kubernetes-security,
      kubernetes-validate,
      python-deploy,
      docker-deploy,
    ]
  runs-on: ubuntu-latest
  environment: production
  steps:
    - uses: actions/checkout@v4
    # Download artifacts from all upstream build jobs
    - name: Download Lambda package from Python build
      uses: actions/download-artifact@v4
      with:
        name: lambda-package
        path: ./lambda-package
    - name: Download Docker image info from Docker build
      uses: actions/download-artifact@v4
      with:
        name: docker-image
        path: ./docker-image
```

**Important**: When multiple dependencies exist, download artifacts from all upstream build jobs. Each build job uploads its artifacts independently, so you can download them all in the downstream deploy job.

---

## Artifact Passing Methods

**For Unified Workflow**: All artifact passing happens within the same workflow using GitHub Actions artifacts.

### Method 1: GitHub Actions Artifacts (MANDATORY for unified workflow)

- Use `actions/upload-artifact@v4` in build jobs
- Use `actions/download-artifact@v4` in deploy jobs
- Works within same workflow run (jobs in same workflow)
- Limited retention (default 1 day, configurable up to 90 days)
- **This is the ONLY method needed for unified workflow architecture**

**Pattern**:

1. **Build Job** (upstream): Upload artifact

   ```yaml
   python-build:
     needs: [python-lint, python-security, python-test]
     steps:
       - name: Upload Lambda package
         uses: actions/upload-artifact@v4
         with:
           name: lambda-package
           path: lambda-package.zip
   ```

2. **Deploy Job** (downstream): Download artifact
   ```yaml
   terraform-deploy:
     needs:
       [terraform-lint, terraform-security, terraform-validate, python-deploy]
     steps:
       - name: Download Lambda package
         uses: actions/download-artifact@v4
         with:
           name: lambda-package
           path: ./lambda-package
   ```

### Alternative Methods (Only if needed for special cases)

### Method 2: S3/Storage (For long retention or external access)

- Upload to S3 bucket with predictable naming
- Download from S3 in deploy job
- Use when: Artifacts need longer retention OR external systems need access

### Method 3: Container Registry (For Docker images)

- Push Docker images with tags to registry (ECR, Docker Hub, etc.)
- Reference image tags in deploy jobs
- Use when: Artifact is a Docker/container image

### Method 4: Environment Variables/Job Outputs (For artifact metadata)

- Pass artifact paths/URLs via job outputs
- Use when: Only artifact location/URL needs to be passed (not the artifact itself)

---

## Single Production Workflow Considerations

- Artifact names should be simple and consistent: `lambda-package` (no environment suffix)
- Workflow trigger: `push` to `main` branch only and `workflow_dispatch` for manual execution
- Use appropriate retention for artifacts (1-7 days typically sufficient)
- All deploy jobs use `environment: production` (single production environment)
- Job dependencies (`needs:`) enforce execution order
- Artifacts are passed between jobs in the same workflow (no cross-workflow downloads needed)

---

## Best Practices

1. **Always verify artifacts exist** before using them in deployment steps
2. **Use consistent artifact naming** to avoid conflicts
3. **Handle errors gracefully** with `continue-on-error` and verification steps
4. **Document artifact paths** in workflow comments for maintainability
5. **Use S3/Storage for multiple dependencies** to avoid complexity
6. **Test artifact passing** before deployment

---

## Troubleshooting

- **Artifact not found**: Check artifact name matches exactly (case-sensitive) between upload and download
- **Download fails**: Verify artifact was uploaded in upstream build job and job dependency (`needs:`) is correct
- **Path mismatch**: Verify artifact is placed where deployment code expects it
- **Job runs before dependency**: Verify `needs:` array includes all required upstream jobs
- **Multiple dependencies**: Download all artifacts from their respective build jobs in the deploy job

See `workflow-common-issues.mdc` for more troubleshooting guidance.
